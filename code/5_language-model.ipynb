{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/a_type_readme.gif\" style=\"float:right ; margin: 10px ; width:300px;\"> \n",
    "<h1><left>NLP Project</left></h1>\n",
    "<h4><left>Using Natural Language Processing to better understand Depression & Anxiety</left></h4>\n",
    "___\n",
    "\n",
    "## 3. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import core, array\n",
    "assert np.__version__ == \"1.19.5\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "# from gensim.models import Word2Vec\n",
    "assert gensim.__version__ == \"4.0.1\"\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from pickle import dump, load\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from time import time \n",
    "\n",
    "from random import randint\n",
    "\n",
    "import logging \n",
    "\n",
    "import multiprocessing\n",
    " \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"../language_model.log\",\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    filemode='w')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def print_time(intput_str, start_time=0):\n",
    "    print(\"{}: {} min\".format(input_str, round((time() - start_time) / 60, 2)))\n",
    "    \n",
    "# #Setting the threshold of logger to DEBUG\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "  \n",
    "# #Test messages\n",
    "# logger.debug(\"Harmless debug Message\")\n",
    "# logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1930 entries, 0 to 1929\n",
      "Data columns (total 13 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   title                      1930 non-null   object\n",
      " 1   selftext                   1930 non-null   object\n",
      " 2   author                     1930 non-null   object\n",
      " 3   score                      1930 non-null   int64 \n",
      " 4   num_comments               1930 non-null   int64 \n",
      " 5   is_anxiety                 1930 non-null   int64 \n",
      " 6   url                        1930 non-null   object\n",
      " 7   selftext_clean             1930 non-null   object\n",
      " 8   selftext_broken_sentences  1930 non-null   object\n",
      " 9   selftext_broken_words      1930 non-null   object\n",
      " 10  title_clean                1930 non-null   object\n",
      " 11  author_clean               1930 non-null   object\n",
      " 12  megatext_clean             1930 non-null   object\n",
      "dtypes: int64(3), object(10)\n",
      "memory usage: 196.1+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_anxiety</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>selftext_broken_sentences</th>\n",
       "      <th>selftext_broken_words</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>2319</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>['we understand that most people who reply imm...</td>\n",
       "      <td>['understand', 'people', 'reply', 'immediately...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regular Check-In Post, with important reminder...</td>\n",
       "      <td>Welcome to /r/depression's check-in post - a p...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>312</td>\n",
       "      <td>1136</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/m...</td>\n",
       "      <td>welcome r depression check post place take mom...</td>\n",
       "      <td>[\"welcome to /r/depression's check-in post - a...</td>\n",
       "      <td>['welcome', 'r', 'depression', 'check', 'post'...</td>\n",
       "      <td>regular check post important reminder private ...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>sql witch welcome r depression check post plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Low</td>\n",
       "      <td>I'm so low rn I can't even type anything coher...</td>\n",
       "      <td>RagingFlock89</td>\n",
       "      <td>263</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/n...</td>\n",
       "      <td>low rn even type anything coherent want expres...</td>\n",
       "      <td>[\"i'm so low rn i can't even type anything coh...</td>\n",
       "      <td>['low', 'rn', 'even', 'type', 'anything', 'coh...</td>\n",
       "      <td>low</td>\n",
       "      <td>raging flock 89</td>\n",
       "      <td>raging flock 89 low rn even type anything cohe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "1  Regular Check-In Post, with important reminder...   \n",
       "2                                                Low   \n",
       "\n",
       "                                            selftext         author  score  \\\n",
       "0  We understand that most people who reply immed...       SQLwitch   2319   \n",
       "1  Welcome to /r/depression's check-in post - a p...       SQLwitch    312   \n",
       "2  I'm so low rn I can't even type anything coher...  RagingFlock89    263   \n",
       "\n",
       "   num_comments  is_anxiety  \\\n",
       "0           175           0   \n",
       "1          1136           0   \n",
       "2            43           0   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/depression/comments/d...   \n",
       "1  https://www.reddit.com/r/depression/comments/m...   \n",
       "2  https://www.reddit.com/r/depression/comments/n...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "1  welcome r depression check post place take mom...   \n",
       "2  low rn even type anything coherent want expres...   \n",
       "\n",
       "                           selftext_broken_sentences  \\\n",
       "0  ['we understand that most people who reply imm...   \n",
       "1  [\"welcome to /r/depression's check-in post - a...   \n",
       "2  [\"i'm so low rn i can't even type anything coh...   \n",
       "\n",
       "                               selftext_broken_words  \\\n",
       "0  ['understand', 'people', 'reply', 'immediately...   \n",
       "1  ['welcome', 'r', 'depression', 'check', 'post'...   \n",
       "2  ['low', 'rn', 'even', 'type', 'anything', 'coh...   \n",
       "\n",
       "                                         title_clean     author_clean  \\\n",
       "0  broken least understood rule helper may invite...        sql witch   \n",
       "1  regular check post important reminder private ...        sql witch   \n",
       "2                                                low  raging flock 89   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  \n",
       "1  sql witch welcome r depression check post plac...  \n",
       "2  raging flock 89 low rn even type anything cohe...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = pd.read_csv('../data/data_for_model.csv', keep_default_na=False)\n",
    "print(model_data.info())\n",
    "model_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = \"selftext_clean\"\n",
    "# model_data[data_column][0]\n",
    "# model_data[\"megatext_clean\"]\n",
    "# sents = [eval(sent) for sent in model_data[\"selftext_broken_words\"]]\n",
    "# dp = model_data[\"selftext_broken_words\"].tolist()[1]\n",
    "# literal_eval(dp)\n",
    "# model_data[\"megatext_clean\"].to_csv(data_path, header=None, index=None, sep='\\t', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_anxiety</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>selftext_broken_sentences</th>\n",
       "      <th>selftext_broken_words</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>2319</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>['we understand that most people who reply imm...</td>\n",
       "      <td>['understand', 'people', 'reply', 'immediately...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regular Check-In Post, with important reminder...</td>\n",
       "      <td>Welcome to /r/depression's check-in post - a p...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>312</td>\n",
       "      <td>1136</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/m...</td>\n",
       "      <td>welcome r depression check post place take mom...</td>\n",
       "      <td>[\"welcome to /r/depression's check-in post - a...</td>\n",
       "      <td>['welcome', 'r', 'depression', 'check', 'post'...</td>\n",
       "      <td>regular check post important reminder private ...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>sql witch welcome r depression check post plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Low</td>\n",
       "      <td>I'm so low rn I can't even type anything coher...</td>\n",
       "      <td>RagingFlock89</td>\n",
       "      <td>263</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/n...</td>\n",
       "      <td>low rn even type anything coherent want expres...</td>\n",
       "      <td>[\"i'm so low rn i can't even type anything coh...</td>\n",
       "      <td>['low', 'rn', 'even', 'type', 'anything', 'coh...</td>\n",
       "      <td>low</td>\n",
       "      <td>raging flock 89</td>\n",
       "      <td>raging flock 89 low rn even type anything cohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I’m always amazed at how much energy healthy p...</td>\n",
       "      <td>When I wake up after 8 hours of decent sleep I...</td>\n",
       "      <td>cezzzie</td>\n",
       "      <td>1281</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/n...</td>\n",
       "      <td>wake 8 hour decent sleep still exhausted day e...</td>\n",
       "      <td>['when i wake up after 8 hours of decent sleep...</td>\n",
       "      <td>['wake', '8', 'hour', 'decent', 'sleep', 'stil...</td>\n",
       "      <td>always amazed much energy healthy people</td>\n",
       "      <td>ce zzz ie</td>\n",
       "      <td>ce zzz ie wake 8 hour decent sleep still exhau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30 and never lived a day in my life</td>\n",
       "      <td>I guess i have always been depressed but never...</td>\n",
       "      <td>ApprehensiveYou2385</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/n...</td>\n",
       "      <td>guess always depressed never really thought mu...</td>\n",
       "      <td>['i guess i have always been depressed but nev...</td>\n",
       "      <td>['guess', 'always', 'depressed', 'never', 'rea...</td>\n",
       "      <td>30 never lived day life</td>\n",
       "      <td>apprehensive 2385</td>\n",
       "      <td>apprehensive 2385 guess always depressed never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>Any tips on how not to panic during a midterm?</td>\n",
       "      <td>I have an Applied Statics midterm tomorrow. I ...</td>\n",
       "      <td>Anomalistic_Username</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/n3p5...</td>\n",
       "      <td>applied static midterm tomorrow already failed...</td>\n",
       "      <td>['i have an applied statics midterm tomorrow.'...</td>\n",
       "      <td>['applied', 'static', 'midterm', 'tomorrow', '...</td>\n",
       "      <td>tip panic midterm</td>\n",
       "      <td>mali stic username</td>\n",
       "      <td>mali stic username applied static midterm tomo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>I find myself apologizing really often, checki...</td>\n",
       "      <td>I've recently decided to stop smoking weed (so...</td>\n",
       "      <td>zedhenson</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/n3n1...</td>\n",
       "      <td>recently decided stop smoking weed socially th...</td>\n",
       "      <td>[\"i've recently decided to stop smoking weed (...</td>\n",
       "      <td>['recently', 'decided', 'stop', 'smoking', 'we...</td>\n",
       "      <td>find apologizing really often checking behavior</td>\n",
       "      <td>zed henson</td>\n",
       "      <td>zed henson recently decided stop smoking weed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>I typed out my anxiety attack and thought I sh...</td>\n",
       "      <td>I recently got into a little habit where when ...</td>\n",
       "      <td>Tree-Nui-Tee</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/n3oz...</td>\n",
       "      <td>recently got little habit thing get overwhelmi...</td>\n",
       "      <td>['i recently got into a little habit where whe...</td>\n",
       "      <td>['recently', 'got', 'little', 'habit', 'thing'...</td>\n",
       "      <td>typed anxiety attack thought share</td>\n",
       "      <td>tree nui tee</td>\n",
       "      <td>tree nui tee recently got little habit thing g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>something happened that just triggered my anxi...</td>\n",
       "      <td>i need someone to vent to please</td>\n",
       "      <td>Capzfan5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/n3gc...</td>\n",
       "      <td>need someone vent please</td>\n",
       "      <td>['i need someone to vent to please']</td>\n",
       "      <td>['need', 'someone', 'vent', 'please']</td>\n",
       "      <td>something happened triggered anxiety really bad</td>\n",
       "      <td>cap z fan 5</td>\n",
       "      <td>cap z fan 5 need someone vent please something...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>Fear of Unknown Person</td>\n",
       "      <td>Everytime I go to my Grandma's old house (she'...</td>\n",
       "      <td>PanamaPhys_</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/n3ot...</td>\n",
       "      <td>everytime go grandma old house still alive ass...</td>\n",
       "      <td>[\"everytime i go to my grandma's old house (sh...</td>\n",
       "      <td>['everytime', 'go', 'grandma', 'old', 'house',...</td>\n",
       "      <td>fear unknown person</td>\n",
       "      <td>panama ph</td>\n",
       "      <td>panama ph everytime go grandma old house still...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1930 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     Our most-broken and least-understood rules is ...   \n",
       "1     Regular Check-In Post, with important reminder...   \n",
       "2                                                   Low   \n",
       "3     I’m always amazed at how much energy healthy p...   \n",
       "4                   30 and never lived a day in my life   \n",
       "...                                                 ...   \n",
       "1925     Any tips on how not to panic during a midterm?   \n",
       "1926  I find myself apologizing really often, checki...   \n",
       "1927  I typed out my anxiety attack and thought I sh...   \n",
       "1928  something happened that just triggered my anxi...   \n",
       "1929                             Fear of Unknown Person   \n",
       "\n",
       "                                               selftext                author  \\\n",
       "0     We understand that most people who reply immed...              SQLwitch   \n",
       "1     Welcome to /r/depression's check-in post - a p...              SQLwitch   \n",
       "2     I'm so low rn I can't even type anything coher...         RagingFlock89   \n",
       "3     When I wake up after 8 hours of decent sleep I...               cezzzie   \n",
       "4     I guess i have always been depressed but never...   ApprehensiveYou2385   \n",
       "...                                                 ...                   ...   \n",
       "1925  I have an Applied Statics midterm tomorrow. I ...  Anomalistic_Username   \n",
       "1926  I've recently decided to stop smoking weed (so...             zedhenson   \n",
       "1927  I recently got into a little habit where when ...          Tree-Nui-Tee   \n",
       "1928                   i need someone to vent to please              Capzfan5   \n",
       "1929  Everytime I go to my Grandma's old house (she'...           PanamaPhys_   \n",
       "\n",
       "      score  num_comments  is_anxiety  \\\n",
       "0      2319           175           0   \n",
       "1       312          1136           0   \n",
       "2       263            43           0   \n",
       "3      1281           120           0   \n",
       "4        36             5           0   \n",
       "...     ...           ...         ...   \n",
       "1925      2             0           1   \n",
       "1926      3             1           1   \n",
       "1927      2             4           1   \n",
       "1928      8             5           1   \n",
       "1929      2             0           1   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://www.reddit.com/r/depression/comments/d...   \n",
       "1     https://www.reddit.com/r/depression/comments/m...   \n",
       "2     https://www.reddit.com/r/depression/comments/n...   \n",
       "3     https://www.reddit.com/r/depression/comments/n...   \n",
       "4     https://www.reddit.com/r/depression/comments/n...   \n",
       "...                                                 ...   \n",
       "1925  https://www.reddit.com/r/Anxiety/comments/n3p5...   \n",
       "1926  https://www.reddit.com/r/Anxiety/comments/n3n1...   \n",
       "1927  https://www.reddit.com/r/Anxiety/comments/n3oz...   \n",
       "1928  https://www.reddit.com/r/Anxiety/comments/n3gc...   \n",
       "1929  https://www.reddit.com/r/Anxiety/comments/n3ot...   \n",
       "\n",
       "                                         selftext_clean  \\\n",
       "0     understand people reply immediately op invitat...   \n",
       "1     welcome r depression check post place take mom...   \n",
       "2     low rn even type anything coherent want expres...   \n",
       "3     wake 8 hour decent sleep still exhausted day e...   \n",
       "4     guess always depressed never really thought mu...   \n",
       "...                                                 ...   \n",
       "1925  applied static midterm tomorrow already failed...   \n",
       "1926  recently decided stop smoking weed socially th...   \n",
       "1927  recently got little habit thing get overwhelmi...   \n",
       "1928                           need someone vent please   \n",
       "1929  everytime go grandma old house still alive ass...   \n",
       "\n",
       "                              selftext_broken_sentences  \\\n",
       "0     ['we understand that most people who reply imm...   \n",
       "1     [\"welcome to /r/depression's check-in post - a...   \n",
       "2     [\"i'm so low rn i can't even type anything coh...   \n",
       "3     ['when i wake up after 8 hours of decent sleep...   \n",
       "4     ['i guess i have always been depressed but nev...   \n",
       "...                                                 ...   \n",
       "1925  ['i have an applied statics midterm tomorrow.'...   \n",
       "1926  [\"i've recently decided to stop smoking weed (...   \n",
       "1927  ['i recently got into a little habit where whe...   \n",
       "1928               ['i need someone to vent to please']   \n",
       "1929  [\"everytime i go to my grandma's old house (sh...   \n",
       "\n",
       "                                  selftext_broken_words  \\\n",
       "0     ['understand', 'people', 'reply', 'immediately...   \n",
       "1     ['welcome', 'r', 'depression', 'check', 'post'...   \n",
       "2     ['low', 'rn', 'even', 'type', 'anything', 'coh...   \n",
       "3     ['wake', '8', 'hour', 'decent', 'sleep', 'stil...   \n",
       "4     ['guess', 'always', 'depressed', 'never', 'rea...   \n",
       "...                                                 ...   \n",
       "1925  ['applied', 'static', 'midterm', 'tomorrow', '...   \n",
       "1926  ['recently', 'decided', 'stop', 'smoking', 'we...   \n",
       "1927  ['recently', 'got', 'little', 'habit', 'thing'...   \n",
       "1928              ['need', 'someone', 'vent', 'please']   \n",
       "1929  ['everytime', 'go', 'grandma', 'old', 'house',...   \n",
       "\n",
       "                                            title_clean        author_clean  \\\n",
       "0     broken least understood rule helper may invite...           sql witch   \n",
       "1     regular check post important reminder private ...           sql witch   \n",
       "2                                                   low     raging flock 89   \n",
       "3              always amazed much energy healthy people           ce zzz ie   \n",
       "4                               30 never lived day life   apprehensive 2385   \n",
       "...                                                 ...                 ...   \n",
       "1925                                  tip panic midterm  mali stic username   \n",
       "1926    find apologizing really often checking behavior          zed henson   \n",
       "1927                 typed anxiety attack thought share        tree nui tee   \n",
       "1928    something happened triggered anxiety really bad         cap z fan 5   \n",
       "1929                                fear unknown person           panama ph   \n",
       "\n",
       "                                         megatext_clean  \n",
       "0     sql witch understand people reply immediately ...  \n",
       "1     sql witch welcome r depression check post plac...  \n",
       "2     raging flock 89 low rn even type anything cohe...  \n",
       "3     ce zzz ie wake 8 hour decent sleep still exhau...  \n",
       "4     apprehensive 2385 guess always depressed never...  \n",
       "...                                                 ...  \n",
       "1925  mali stic username applied static midterm tomo...  \n",
       "1926  zed henson recently decided stop smoking weed ...  \n",
       "1927  tree nui tee recently got little habit thing g...  \n",
       "1928  cap z fan 5 need someone vent please something...  \n",
       "1929  panama ph everytime go grandma old house still...  \n",
       "\n",
       "[1930 rows x 13 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_token_seqs(data, length=20+1):\n",
    "    sequences = list()\n",
    "    \n",
    "    for record in data:\n",
    "        tokens = word_tokenize(record)\n",
    "        \n",
    "        for i in range(length, len(tokens)):\n",
    "            seq = tokens[i-length: i]\n",
    "            assert len(seq) == length, (length, len(seq), seq)\n",
    "            line = ' '.join(seq)\n",
    "            sequences.append(line)\n",
    "        \n",
    "#     if len(sequences)== 0:\n",
    "#         print(tokens)\n",
    "#     print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LM_A(vocab_size, seq_length, summary=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    if summary:\n",
    "        print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model(label, data, n_seq_words, EPOCHS=100, BATCH=128):\n",
    "    model_path = '../models/{}.language_model.h5'.format(label)\n",
    "    tokenizer_path = '../models/{}.language_model.tokenizer.pkl'.format(label)\n",
    "    processed_data_path = '../data/{}.language_model.processed_data.txt'.format(label)\n",
    "    \n",
    "    # ------------------------------ PREPARE DATA -----------------------------------\n",
    "    processed_data = organize_token_seqs(data, n_seq_words)\n",
    "    save_doc(processed_data, processed_data_path)\n",
    "    \n",
    "#     doc = load_doc(processed_data_path)\n",
    "#     processed_data = doc.split('\\n')\n",
    "#     print(\"\\nword sequences\\n\", processed_data[:5])\n",
    "    \n",
    "    \n",
    "    # integer encode sequences of words\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(processed_data)\n",
    "    unchecked_sequences = tokenizer.texts_to_sequences(processed_data)\n",
    "    print('\\nTotal before Sequences: %d' % len(unchecked_sequences))\n",
    "    print(\"int sequences lengths\", set([len(seq) for seq in unchecked_sequences]))    \n",
    "    #     print(\"\\nint sequences\\n\", sequences[:5])\n",
    "    \n",
    "    \n",
    "    sequences = []\n",
    "    for seq in unchecked_sequences:\n",
    "        if len(seq) == n_seq_words:\n",
    "            sequences.append(seq)\n",
    "    print('\\nTotal after Sequences: %d' % len(sequences))\n",
    "    print(\"int sequences lengths\", set([len(seq) for seq in sequences]))    \n",
    "    \n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(\"\\nvocab_size\", vocab_size)\n",
    "    \n",
    "    # separate into input and output\n",
    "    sequences = array(sequences)\n",
    "#     print(\"\\nsequences\", sequences)\n",
    "    print(\"\\nsequences.shape\", sequences.shape)\n",
    "    \n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    print(\"X\\n\", X)\n",
    "    print(\"y\\n\", y)\n",
    "    \n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    seq_length = X.shape[1]+1\n",
    "#     seq_length =len(X[0])\n",
    "    print(\"seq_length\", seq_length)\n",
    "    \n",
    "    # ------------------------------ TRAIN MODEL -----------------------------------\n",
    "    model = build_LM_A(vocab_size, seq_length, summary=True)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(X, y, batch_size=BATCH, epochs=EPOCHS, verbose=1) \n",
    "    \n",
    "    model.save(model_path)\n",
    "    dump(tokenizer, open(tokenizer_path, 'wb'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = []\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed_text\n",
      " usually fall asleep lay run different communication scenario head different people play could happen last night wa different one day may wake told wake hope kid know love much hope loved wish could better hope one find hope could find another figure love depend better could depend really feeling really done life sure going next man fucking tired living\n",
      "\n",
      "i exp: meaning regard depressed inside want chaos anyway feed fault even followed class know assume example suffering want good want understand\n",
      "seed_text\n",
      " depressed year take med go therapy best follow every advice doctor feel bad time anymore functional still depressive thought motivation anything reason live doe ever get better long taking med actually started live please give hope\n",
      "\n",
      "i exp: assure post rather want free lie someone together invalidate shes fuckkkk feel like stressed time spend different diagnosed depression mild\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "716",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\yegmo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2898\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 716",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-636b2e69f6b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mseed_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"seed_text\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_text\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yegmo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yegmo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yegmo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2898\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2900\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2902\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 716"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH = 128\n",
    "n_seq_words = 20\n",
    "exp_count = 2\n",
    "\n",
    "labels = {'depression': 0, 'anxiety': 1}\n",
    "results = []\n",
    "\n",
    "for label_str, label_int in labels.items():\n",
    "    data = model_data[model_data[\"is_anxiety\"] == label_int][data_column]\n",
    "#     model = language_model(label_str, data, n_seq_words, EPOCHS, BATCH)\n",
    "    \n",
    "    model_path = '../models/{}.language_model.h5'.format(label_str)\n",
    "    tokenizer_path = '../models/{}.language_model.tokenizer.pkl'.format(label_str)\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load(open(tokenizer_path, 'rb'))\n",
    "    \n",
    "    \n",
    "    for i in range(exp_count):\n",
    "        seed_text = data[randint(0, len(data))]\n",
    "        print(\"seed_text\\n\", seed_text + '\\n')\n",
    "        \n",
    "#         generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "        generated = generate_seq(model, tokenizer, n_seq_words, seed_text, n_seq_words)\n",
    "        print(\"i exp:\", generated)\n",
    "        \n",
    "        model_results = {}\n",
    "        model_results[\"seed_text\"] = seed_text\n",
    "        model_results[\"generated\"] = generated\n",
    "        model_results[\"is_anxiety\"] = label_int\n",
    "        model_results[\"is_anxiety\"] = n_seq_words\n",
    "        results.append(model_results) \n",
    "    \n",
    "return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "964d23c0451dcf6eb9653ab29bf1b69d4b9cf2f8bbce59b0642e3dd73cc18be8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
