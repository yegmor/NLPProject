{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/a_type_readme.gif\" style=\"float:right ; margin: 10px ; width:300px;\"> \n",
    "<h1><left>NLP Project</left></h1>\n",
    "<h4><left>Using Natural Language Processing to better understand Depression & Anxiety</left></h4>\n",
    "___\n",
    "\n",
    "## 3. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import core, array\n",
    "assert np.__version__ == \"1.19.5\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "# from gensim.models import Word2Vec\n",
    "assert gensim.__version__ == \"4.0.1\"\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from pickle import dump, load\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from time import time \n",
    "\n",
    "from random import randint\n",
    "\n",
    "import logging \n",
    "\n",
    "import multiprocessing\n",
    " \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"../logs/5_language-model.log\",\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    filemode='w')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def print_time(intput_str, start_time=0):\n",
    "    print(\"{}: {} min\".format(input_str, round((time() - start_time) / 60, 2)))\n",
    "    \n",
    "# #Setting the threshold of logger to DEBUG\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "  \n",
    "# #Test messages\n",
    "# logger.debug(\"Harmless debug Message\")\n",
    "# logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1930 entries, 0 to 1929\n",
      "Data columns (total 13 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   title                      1930 non-null   object\n",
      " 1   selftext                   1930 non-null   object\n",
      " 2   author                     1930 non-null   object\n",
      " 3   score                      1930 non-null   int64 \n",
      " 4   num_comments               1930 non-null   int64 \n",
      " 5   is_anxiety                 1930 non-null   int64 \n",
      " 6   url                        1930 non-null   object\n",
      " 7   selftext_clean             1930 non-null   object\n",
      " 8   selftext_broken_sentences  1930 non-null   object\n",
      " 9   selftext_broken_words      1930 non-null   object\n",
      " 10  title_clean                1930 non-null   object\n",
      " 11  author_clean               1930 non-null   object\n",
      " 12  megatext_clean             1930 non-null   object\n",
      "dtypes: int64(3), object(10)\n",
      "memory usage: 196.1+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_anxiety</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>selftext_broken_sentences</th>\n",
       "      <th>selftext_broken_words</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>2319</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>['we understand that most people who reply imm...</td>\n",
       "      <td>['understand', 'people', 'reply', 'immediately...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regular Check-In Post, with important reminder...</td>\n",
       "      <td>Welcome to /r/depression's check-in post - a p...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>312</td>\n",
       "      <td>1136</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/m...</td>\n",
       "      <td>welcome r depression check post place take mom...</td>\n",
       "      <td>[\"welcome to /r/depression's check-in post - a...</td>\n",
       "      <td>['welcome', 'r', 'depression', 'check', 'post'...</td>\n",
       "      <td>regular check post important reminder private ...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>sql witch welcome r depression check post plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Low</td>\n",
       "      <td>I'm so low rn I can't even type anything coher...</td>\n",
       "      <td>RagingFlock89</td>\n",
       "      <td>263</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/n...</td>\n",
       "      <td>low rn even type anything coherent want expres...</td>\n",
       "      <td>[\"i'm so low rn i can't even type anything coh...</td>\n",
       "      <td>['low', 'rn', 'even', 'type', 'anything', 'coh...</td>\n",
       "      <td>low</td>\n",
       "      <td>raging flock 89</td>\n",
       "      <td>raging flock 89 low rn even type anything cohe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "1  Regular Check-In Post, with important reminder...   \n",
       "2                                                Low   \n",
       "\n",
       "                                            selftext         author  score  \\\n",
       "0  We understand that most people who reply immed...       SQLwitch   2319   \n",
       "1  Welcome to /r/depression's check-in post - a p...       SQLwitch    312   \n",
       "2  I'm so low rn I can't even type anything coher...  RagingFlock89    263   \n",
       "\n",
       "   num_comments  is_anxiety  \\\n",
       "0           175           0   \n",
       "1          1136           0   \n",
       "2            43           0   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/depression/comments/d...   \n",
       "1  https://www.reddit.com/r/depression/comments/m...   \n",
       "2  https://www.reddit.com/r/depression/comments/n...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "1  welcome r depression check post place take mom...   \n",
       "2  low rn even type anything coherent want expres...   \n",
       "\n",
       "                           selftext_broken_sentences  \\\n",
       "0  ['we understand that most people who reply imm...   \n",
       "1  [\"welcome to /r/depression's check-in post - a...   \n",
       "2  [\"i'm so low rn i can't even type anything coh...   \n",
       "\n",
       "                               selftext_broken_words  \\\n",
       "0  ['understand', 'people', 'reply', 'immediately...   \n",
       "1  ['welcome', 'r', 'depression', 'check', 'post'...   \n",
       "2  ['low', 'rn', 'even', 'type', 'anything', 'coh...   \n",
       "\n",
       "                                         title_clean     author_clean  \\\n",
       "0  broken least understood rule helper may invite...        sql witch   \n",
       "1  regular check post important reminder private ...        sql witch   \n",
       "2                                                low  raging flock 89   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  \n",
       "1  sql witch welcome r depression check post plac...  \n",
       "2  raging flock 89 low rn even type anything cohe...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = pd.read_csv('../data/data_for_model.csv', keep_default_na=False)\n",
    "print(model_data.info())\n",
    "data_column = \"selftext_clean\"\n",
    "model_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_token_seqs(data, length=20+1):\n",
    "    sequences = list()\n",
    "    \n",
    "    for record in data:\n",
    "        tokens = word_tokenize(record)\n",
    "        \n",
    "        for i in range(length, len(tokens)):\n",
    "            seq = tokens[i-length: i]\n",
    "            assert len(seq) == length, (length, len(seq), seq)\n",
    "            line = ' '.join(seq)\n",
    "            sequences.append(line)\n",
    "        \n",
    "#     if len(sequences)== 0:\n",
    "#         print(tokens)\n",
    "#     print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LM_A(vocab_size, seq_length, summary=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    if summary:\n",
    "        print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model(label, data, n_seq_words, EPOCHS=100, BATCH=128):\n",
    "    model_path = '../models/{}.language_model.h5'.format(label)\n",
    "    tokenizer_path = '../models/{}.language_model.tokenizer.pkl'.format(label)\n",
    "    processed_data_path = '../data/{}.language_model.processed_data.txt'.format(label)\n",
    "    \n",
    "    # ------------------------------ PREPARE DATA -----------------------------------\n",
    "    processed_data = organize_token_seqs(data, n_seq_words)\n",
    "    save_doc(processed_data, processed_data_path)\n",
    "    \n",
    "#     doc = load_doc(processed_data_path)\n",
    "#     processed_data = doc.split('\\n')\n",
    "#     print(\"\\nword sequences\\n\", processed_data[:5])\n",
    "    \n",
    "    \n",
    "    # integer encode sequences of words\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(processed_data)\n",
    "    unchecked_sequences = tokenizer.texts_to_sequences(processed_data)\n",
    "    print('\\nTotal before Sequences: %d' % len(unchecked_sequences))\n",
    "    print(\"int sequences lengths\", set([len(seq) for seq in unchecked_sequences]))    \n",
    "    #     print(\"\\nint sequences\\n\", sequences[:5])\n",
    "    \n",
    "    \n",
    "    sequences = []\n",
    "    for seq in unchecked_sequences:\n",
    "        if len(seq) == n_seq_words:\n",
    "            sequences.append(seq)\n",
    "    print('\\nTotal after Sequences: %d' % len(sequences))\n",
    "    print(\"int sequences lengths\", set([len(seq) for seq in sequences]))    \n",
    "    \n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(\"\\nvocab_size\", vocab_size)\n",
    "    \n",
    "    # separate into input and output\n",
    "    sequences = array(sequences)\n",
    "#     print(\"\\nsequences\", sequences)\n",
    "    print(\"\\nsequences.shape\", sequences.shape)\n",
    "    \n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    print(\"X\\n\", X)\n",
    "    print(\"y\\n\", y)\n",
    "    \n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    seq_length = X.shape[1]+1\n",
    "#     seq_length =len(X[0])\n",
    "    print(\"seq_length\", seq_length)\n",
    "    \n",
    "    # ------------------------------ TRAIN MODEL -----------------------------------\n",
    "    model = build_LM_A(vocab_size, seq_length, summary=True)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(X, y, batch_size=BATCH, epochs=EPOCHS, verbose=1) \n",
    "    \n",
    "    model.save(model_path)\n",
    "    dump(tokenizer, open(tokenizer_path, 'wb'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = []\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depression total data = 932\n",
      "anxiety total data = 998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed_text</th>\n",
       "      <th>generated</th>\n",
       "      <th>is_anxiety</th>\n",
       "      <th>n_seq_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>firmly believe social anxiety sole reason deal...</td>\n",
       "      <td>enjoy water mate major gave center shower deal...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>much ha happened last year covid ha definitely...</td>\n",
       "      <td>away feel like shallowest blandest person thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>think everyone understands fear missing fomo u...</td>\n",
       "      <td>neighbor passionate therapy deeper word trigge...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>took first dose lexapro today doctor advised s...</td>\n",
       "      <td>anxiety fear thought way something wa talking ...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           seed_text  \\\n",
       "0  firmly believe social anxiety sole reason deal...   \n",
       "1  much ha happened last year covid ha definitely...   \n",
       "2  think everyone understands fear missing fomo u...   \n",
       "3  took first dose lexapro today doctor advised s...   \n",
       "\n",
       "                                           generated  is_anxiety  n_seq_words  \n",
       "0  enjoy water mate major gave center shower deal...           0           20  \n",
       "1  away feel like shallowest blandest person thin...           0           20  \n",
       "2  neighbor passionate therapy deeper word trigge...           1           20  \n",
       "3  anxiety fear thought way something wa talking ...           1           20  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH = 128\n",
    "n_seq_words = 20\n",
    "exp_count = 2\n",
    "\n",
    "labels = {'depression': 0, 'anxiety': 1}\n",
    "results = []\n",
    "\n",
    "for label_str, label_int in labels.items():\n",
    "    data = model_data[model_data[\"is_anxiety\"] == label_int][data_column]\n",
    "#     model = language_model(label_str, data, n_seq_words, EPOCHS, BATCH)\n",
    "    \n",
    "    print(label_str, \"total data =\", len(data))\n",
    "    \n",
    "    model_path = '../models/{}.language_model.h5'.format(label_str)\n",
    "    tokenizer_path = '../models/{}.language_model.tokenizer.pkl'.format(label_str)\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load(open(tokenizer_path, 'rb'))\n",
    "    \n",
    "    \n",
    "    for i in range(exp_count):\n",
    "        seed_text = data[randint(data.index[0], data.index[-1]+1)]\n",
    "\n",
    "#         print(\"seed_text\\n\", seed_text + '\\n')\n",
    "        \n",
    "#         generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "        generated = generate_seq(model, tokenizer, n_seq_words, seed_text, n_seq_words)\n",
    "#         print(\"i exp:\", generated)\n",
    "        \n",
    "        model_results = {}\n",
    "        model_results[\"seed_text\"] = seed_text\n",
    "        model_results[\"generated\"] = generated\n",
    "        model_results[\"is_anxiety\"] = label_int\n",
    "        model_results[\"n_seq_words\"] = n_seq_words\n",
    "        results.append(model_results) \n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.export(results, '../reports/images/normal-lm_examples.png')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "964d23c0451dcf6eb9653ab29bf1b69d4b9cf2f8bbce59b0642e3dd73cc18be8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}