{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/a_type_readme.gif\" style=\"float:right ; margin: 10px ; width:300px;\"> \n",
    "<h1><left>NLP Project</left></h1>\n",
    "<h4><left>Using Natural Language Processing to better understand Depression & Anxiety</left></h4>\n",
    "___\n",
    "\n",
    "## 3. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import core, array\n",
    "assert np.__version__ == \"1.19.5\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import dataframe_image as dfi\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import sentencepiece as spm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from time import time \n",
    "import logging \n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"../logs/4_tokenization.log\",\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    filemode='w',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "def add_time(intput_str, start_time=0):\n",
    "    print(\"{}: {} min\".format(input_str, round((time() - start_time) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1930 entries, 0 to 1929\nData columns (total 13 columns):\n #   Column                     Non-Null Count  Dtype \n---  ------                     --------------  ----- \n 0   title                      1930 non-null   object\n 1   selftext                   1930 non-null   object\n 2   author                     1930 non-null   object\n 3   score                      1930 non-null   int64 \n 4   num_comments               1930 non-null   int64 \n 5   is_anxiety                 1930 non-null   int64 \n 6   url                        1930 non-null   object\n 7   selftext_clean             1930 non-null   object\n 8   selftext_broken_sentences  1930 non-null   object\n 9   selftext_broken_words      1930 non-null   object\n 10  title_clean                1930 non-null   object\n 11  author_clean               1930 non-null   object\n 12  megatext_clean             1930 non-null   object\ndtypes: int64(3), object(10)\nmemory usage: 196.1+ KB\nNone\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "1  Regular Check-In Post, with important reminder...   \n",
       "2                                                Low   \n",
       "\n",
       "                                            selftext         author  score  \\\n",
       "0  We understand that most people who reply immed...       SQLwitch   2319   \n",
       "1  Welcome to /r/depression's check-in post - a p...       SQLwitch    312   \n",
       "2  I'm so low rn I can't even type anything coher...  RagingFlock89    263   \n",
       "\n",
       "   num_comments  is_anxiety  \\\n",
       "0           175           0   \n",
       "1          1136           0   \n",
       "2            43           0   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/depression/comments/d...   \n",
       "1  https://www.reddit.com/r/depression/comments/m...   \n",
       "2  https://www.reddit.com/r/depression/comments/n...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "1  welcome r depression check post place take mom...   \n",
       "2  low rn even type anything coherent want expres...   \n",
       "\n",
       "                           selftext_broken_sentences  \\\n",
       "0  ['we understand that most people who reply imm...   \n",
       "1  [\"welcome to /r/depression's check-in post - a...   \n",
       "2  [\"i'm so low rn i can't even type anything coh...   \n",
       "\n",
       "                               selftext_broken_words  \\\n",
       "0  ['understand', 'people', 'reply', 'immediately...   \n",
       "1  ['welcome', 'r', 'depression', 'check', 'post'...   \n",
       "2  ['low', 'rn', 'even', 'type', 'anything', 'coh...   \n",
       "\n",
       "                                         title_clean     author_clean  \\\n",
       "0  broken least understood rule helper may invite...        sql witch   \n",
       "1  regular check post important reminder private ...        sql witch   \n",
       "2                                                low  raging flock 89   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  \n",
       "1  sql witch welcome r depression check post plac...  \n",
       "2  raging flock 89 low rn even type anything cohe...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>selftext</th>\n      <th>author</th>\n      <th>score</th>\n      <th>num_comments</th>\n      <th>is_anxiety</th>\n      <th>url</th>\n      <th>selftext_clean</th>\n      <th>selftext_broken_sentences</th>\n      <th>selftext_broken_words</th>\n      <th>title_clean</th>\n      <th>author_clean</th>\n      <th>megatext_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Our most-broken and least-understood rules is ...</td>\n      <td>We understand that most people who reply immed...</td>\n      <td>SQLwitch</td>\n      <td>2319</td>\n      <td>175</td>\n      <td>0</td>\n      <td>https://www.reddit.com/r/depression/comments/d...</td>\n      <td>understand people reply immediately op invitat...</td>\n      <td>['we understand that most people who reply imm...</td>\n      <td>['understand', 'people', 'reply', 'immediately...</td>\n      <td>broken least understood rule helper may invite...</td>\n      <td>sql witch</td>\n      <td>sql witch understand people reply immediately ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Regular Check-In Post, with important reminder...</td>\n      <td>Welcome to /r/depression's check-in post - a p...</td>\n      <td>SQLwitch</td>\n      <td>312</td>\n      <td>1136</td>\n      <td>0</td>\n      <td>https://www.reddit.com/r/depression/comments/m...</td>\n      <td>welcome r depression check post place take mom...</td>\n      <td>[\"welcome to /r/depression's check-in post - a...</td>\n      <td>['welcome', 'r', 'depression', 'check', 'post'...</td>\n      <td>regular check post important reminder private ...</td>\n      <td>sql witch</td>\n      <td>sql witch welcome r depression check post plac...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Low</td>\n      <td>I'm so low rn I can't even type anything coher...</td>\n      <td>RagingFlock89</td>\n      <td>263</td>\n      <td>43</td>\n      <td>0</td>\n      <td>https://www.reddit.com/r/depression/comments/n...</td>\n      <td>low rn even type anything coherent want expres...</td>\n      <td>[\"i'm so low rn i can't even type anything coh...</td>\n      <td>['low', 'rn', 'even', 'type', 'anything', 'coh...</td>\n      <td>low</td>\n      <td>raging flock 89</td>\n      <td>raging flock 89 low rn even type anything cohe...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "model_data = pd.read_csv('../data/data_for_model.csv', keep_default_na=False)\n",
    "data_column = \"selftext_clean\"\n",
    "print(model_data.info())\n",
    "model_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenization(data_np, vocab_size):\n",
    "    logging.info(\"In tokenization-> vocab_size={}\".format(vocab_size))    \n",
    "#     train, test = train_test_split(data_np, test_size=0.2, random_state=42)\n",
    "    \n",
    "    i = 0\n",
    "    for train_idx, test_idx in kfold.split(data_np):\n",
    "        i += 1\n",
    "        train_data = data_np[train_idx]\n",
    "        test_data = data_np[test_idx]\n",
    "\n",
    "        train_path = \"../data/tokenization/train_vs{}_i{}.txt\".format(vocab_size, i)\n",
    "        np.savetxt(train_path, train_data, fmt='%s')\n",
    "        \n",
    "        model_prefix = '../models/tokenization/vs{}_i{}'.format(vocab_size, i)\n",
    "        \n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=train_path,\n",
    "            model_prefix=model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            unk_id=3,\n",
    "            model_type='word'\n",
    "        )   \n",
    "        sp = spm.SentencePieceProcessor()   # create an instance; this saves model and .vocab files \n",
    "        sp.load('{}.model'.format(model_prefix))    # loads model\n",
    "\n",
    "    #     data_subwords  = [sp.id_to_piece(piece_id) for piece_id in range(sp.get_piece_size())]          # list of subwords\n",
    "        unks_count = 0\n",
    "        tokens_count = 0\n",
    "\n",
    "        for post in test_data:\n",
    "            sp_encoded = sp.encode_as_ids(post)\n",
    "            # print(sp.encode_as_pieces(post))\n",
    "            tokens_count += len(sp_encoded)\n",
    "            unks_count += sp_encoded.count(3)\n",
    "\n",
    "        percentage = unks_count / tokens_count * 100\n",
    "\n",
    "        results.append({\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"iteration\": i,\n",
    "            \"unks_count\": unks_count,\n",
    "            \"all_tokens_count\": tokens_count,\n",
    "            \"unks_percentage\": percentage\n",
    "        }) \n",
    "\n",
    "        msg = \"\\t vocab_size={} iteration={}: unks_count={}, all_tokens_count={} =>{}%\".format(vocab_size, i, unks_count, tokens_count, percentage)\n",
    "        logging.info(msg)\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\t vocab_size=20 iteration=1: unks_count=3962, all_tokens_count=8302 =>47.72344013490725%\n",
      "\t vocab_size=20 iteration=2: unks_count=4324, all_tokens_count=9085 =>47.59493670886076%\n",
      "\t vocab_size=20 iteration=3: unks_count=4273, all_tokens_count=9001 =>47.47250305521609%\n",
      "\t vocab_size=20 iteration=4: unks_count=4303, all_tokens_count=9033 =>47.6364441492306%\n",
      "\t vocab_size=20 iteration=5: unks_count=4334, all_tokens_count=9145 =>47.392017495899395%\n",
      "\n",
      "\n",
      "\t vocab_size=100 iteration=1: unks_count=6743, all_tokens_count=17627 =>38.2538151699098%\n",
      "\t vocab_size=100 iteration=2: unks_count=7527, all_tokens_count=19958 =>37.7141998196212%\n",
      "\t vocab_size=100 iteration=3: unks_count=7285, all_tokens_count=19052 =>38.237455385261384%\n",
      "\t vocab_size=100 iteration=4: unks_count=7169, all_tokens_count=18789 =>38.155303635105646%\n",
      "\t vocab_size=100 iteration=5: unks_count=7303, all_tokens_count=19214 =>38.00874362444051%\n",
      "\n",
      "\n",
      "\t vocab_size=500 iteration=1: unks_count=6203, all_tokens_count=25842 =>24.003560095967806%\n",
      "\t vocab_size=500 iteration=2: unks_count=7100, all_tokens_count=29575 =>24.00676246830093%\n",
      "\t vocab_size=500 iteration=3: unks_count=7014, all_tokens_count=28609 =>24.51676045999511%\n",
      "\t vocab_size=500 iteration=4: unks_count=6570, all_tokens_count=27662 =>23.750994143590486%\n",
      "\t vocab_size=500 iteration=5: unks_count=6852, all_tokens_count=28512 =>24.031986531986533%\n",
      "\n",
      "\n",
      "\t vocab_size=1500 iteration=1: unks_count=3827, all_tokens_count=28656 =>13.354969290898937%\n",
      "\t vocab_size=1500 iteration=2: unks_count=4527, all_tokens_count=32869 =>13.772855882442423%\n",
      "\t vocab_size=1500 iteration=3: unks_count=4606, all_tokens_count=32262 =>14.276858223296756%\n",
      "\t vocab_size=1500 iteration=4: unks_count=4036, all_tokens_count=30633 =>13.175333790356806%\n",
      "\t vocab_size=1500 iteration=5: unks_count=4304, all_tokens_count=31579 =>13.629310617815637%\n",
      "\n",
      "\n",
      "\t vocab_size=4000 iteration=1: unks_count=1988, all_tokens_count=29463 =>6.747445949156569%\n",
      "\t vocab_size=4000 iteration=2: unks_count=2339, all_tokens_count=33891 =>6.901537281284116%\n",
      "\t vocab_size=4000 iteration=3: unks_count=2374, all_tokens_count=33426 =>7.102255729073177%\n",
      "\t vocab_size=4000 iteration=4: unks_count=2020, all_tokens_count=31508 =>6.411070204392535%\n",
      "\t vocab_size=4000 iteration=5: unks_count=2268, all_tokens_count=32551 =>6.967527879327824%\n",
      "\n",
      "\n",
      "\t vocab_size=9000 iteration=1: unks_count=1120, all_tokens_count=29663 =>3.775747564305701%\n",
      "\t vocab_size=9000 iteration=2: unks_count=1290, all_tokens_count=34149 =>3.7775630325924627%\n",
      "\t vocab_size=9000 iteration=3: unks_count=1375, all_tokens_count=33741 =>4.075160783616372%\n",
      "\t vocab_size=9000 iteration=4: unks_count=1152, all_tokens_count=31708 =>3.633152516715025%\n",
      "\t vocab_size=9000 iteration=5: unks_count=1357, all_tokens_count=32792 =>4.1382044401073435%\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    vocab_size  iteration  unks_count  all_tokens_count  unks_percentage\n",
       "0           20          1        3962              8302        47.723440\n",
       "1           20          2        4324              9085        47.594937\n",
       "2           20          3        4273              9001        47.472503\n",
       "3           20          4        4303              9033        47.636444\n",
       "4           20          5        4334              9145        47.392017\n",
       "5          100          1        6743             17627        38.253815\n",
       "6          100          2        7527             19958        37.714200\n",
       "7          100          3        7285             19052        38.237455\n",
       "8          100          4        7169             18789        38.155304\n",
       "9          100          5        7303             19214        38.008744\n",
       "10         500          1        6203             25842        24.003560\n",
       "11         500          2        7100             29575        24.006762\n",
       "12         500          3        7014             28609        24.516760\n",
       "13         500          4        6570             27662        23.750994\n",
       "14         500          5        6852             28512        24.031987\n",
       "15        1500          1        3827             28656        13.354969\n",
       "16        1500          2        4527             32869        13.772856\n",
       "17        1500          3        4606             32262        14.276858\n",
       "18        1500          4        4036             30633        13.175334\n",
       "19        1500          5        4304             31579        13.629311\n",
       "20        4000          1        1988             29463         6.747446\n",
       "21        4000          2        2339             33891         6.901537\n",
       "22        4000          3        2374             33426         7.102256\n",
       "23        4000          4        2020             31508         6.411070\n",
       "24        4000          5        2268             32551         6.967528\n",
       "25        9000          1        1120             29663         3.775748\n",
       "26        9000          2        1290             34149         3.777563\n",
       "27        9000          3        1375             33741         4.075161\n",
       "28        9000          4        1152             31708         3.633153\n",
       "29        9000          5        1357             32792         4.138204"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vocab_size</th>\n      <th>iteration</th>\n      <th>unks_count</th>\n      <th>all_tokens_count</th>\n      <th>unks_percentage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20</td>\n      <td>1</td>\n      <td>3962</td>\n      <td>8302</td>\n      <td>47.723440</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>2</td>\n      <td>4324</td>\n      <td>9085</td>\n      <td>47.594937</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20</td>\n      <td>3</td>\n      <td>4273</td>\n      <td>9001</td>\n      <td>47.472503</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20</td>\n      <td>4</td>\n      <td>4303</td>\n      <td>9033</td>\n      <td>47.636444</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20</td>\n      <td>5</td>\n      <td>4334</td>\n      <td>9145</td>\n      <td>47.392017</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>100</td>\n      <td>1</td>\n      <td>6743</td>\n      <td>17627</td>\n      <td>38.253815</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>100</td>\n      <td>2</td>\n      <td>7527</td>\n      <td>19958</td>\n      <td>37.714200</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>100</td>\n      <td>3</td>\n      <td>7285</td>\n      <td>19052</td>\n      <td>38.237455</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>100</td>\n      <td>4</td>\n      <td>7169</td>\n      <td>18789</td>\n      <td>38.155304</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>100</td>\n      <td>5</td>\n      <td>7303</td>\n      <td>19214</td>\n      <td>38.008744</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>500</td>\n      <td>1</td>\n      <td>6203</td>\n      <td>25842</td>\n      <td>24.003560</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>500</td>\n      <td>2</td>\n      <td>7100</td>\n      <td>29575</td>\n      <td>24.006762</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>500</td>\n      <td>3</td>\n      <td>7014</td>\n      <td>28609</td>\n      <td>24.516760</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>500</td>\n      <td>4</td>\n      <td>6570</td>\n      <td>27662</td>\n      <td>23.750994</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>500</td>\n      <td>5</td>\n      <td>6852</td>\n      <td>28512</td>\n      <td>24.031987</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1500</td>\n      <td>1</td>\n      <td>3827</td>\n      <td>28656</td>\n      <td>13.354969</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1500</td>\n      <td>2</td>\n      <td>4527</td>\n      <td>32869</td>\n      <td>13.772856</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1500</td>\n      <td>3</td>\n      <td>4606</td>\n      <td>32262</td>\n      <td>14.276858</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1500</td>\n      <td>4</td>\n      <td>4036</td>\n      <td>30633</td>\n      <td>13.175334</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1500</td>\n      <td>5</td>\n      <td>4304</td>\n      <td>31579</td>\n      <td>13.629311</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>4000</td>\n      <td>1</td>\n      <td>1988</td>\n      <td>29463</td>\n      <td>6.747446</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>4000</td>\n      <td>2</td>\n      <td>2339</td>\n      <td>33891</td>\n      <td>6.901537</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>4000</td>\n      <td>3</td>\n      <td>2374</td>\n      <td>33426</td>\n      <td>7.102256</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>4000</td>\n      <td>4</td>\n      <td>2020</td>\n      <td>31508</td>\n      <td>6.411070</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>4000</td>\n      <td>5</td>\n      <td>2268</td>\n      <td>32551</td>\n      <td>6.967528</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>9000</td>\n      <td>1</td>\n      <td>1120</td>\n      <td>29663</td>\n      <td>3.775748</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>9000</td>\n      <td>2</td>\n      <td>1290</td>\n      <td>34149</td>\n      <td>3.777563</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>9000</td>\n      <td>3</td>\n      <td>1375</td>\n      <td>33741</td>\n      <td>4.075161</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>9000</td>\n      <td>4</td>\n      <td>1152</td>\n      <td>31708</td>\n      <td>3.633153</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>9000</td>\n      <td>5</td>\n      <td>1357</td>\n      <td>32792</td>\n      <td>4.138204</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "data_np = model_data[data_column].to_numpy()\n",
    "\n",
    "for vocab_size in [20, 100, 500, 1500, 4000, 9000]:\n",
    "    tokenization(data_np, vocab_size=vocab_size)\n",
    "    print(\"\\n\")\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.export(results, '../reports/images/token_vocab-size.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_tekinzer(data_np, vocab_size):\n",
    "    logging.info(\"In best_tekinzer-> vocab_size={}\".format(vocab_size))    \n",
    "    # train_data, test_data = train_test_split(data_np, test_size=0.2, random_state=42)\n",
    "\n",
    "    # train_data = data_np[train_idx]\n",
    "    # test_data = data_np[test_idx]\n",
    "\n",
    "    data_path = \"../data/tokenization/all_data.txt\"\n",
    "    np.savetxt(data_path, data_np, fmt='%s')\n",
    "    \n",
    "    model_prefix = '../models/tokenization'\n",
    "    \n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=data_path,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        unk_id=3,\n",
    "        model_type='word'\n",
    "    )   \n",
    "    sp = spm.SentencePieceProcessor()   # create an instance; this saves model and .vocab files \n",
    "    sp.load('{}.model'.format(model_prefix))    # loads model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tekinzer(data_np, vocab_size=9000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "964d23c0451dcf6eb9653ab29bf1b69d4b9cf2f8bbce59b0642e3dd73cc18be8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}