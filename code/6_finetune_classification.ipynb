{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/a_type_readme.gif\" style=\"float:right ; margin: 10px ; width:300px;\"> \n",
    "<h1><left>NLP Project</left></h1>\n",
    "<h4><left>Using Natural Language Processing to better understand Depression & Anxiety</left></h4>\n",
    "___\n",
    "\n",
    "## 3. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import core, array\n",
    "# assert np.__version__ == \"1.19.5\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from random import randint\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.set_style(\"darkgrid\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from time import time \n",
    "\n",
    "import logging \n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from datasets import load_metric\n",
    " \n",
    "# !pip install pandas transformers\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import random\n",
    "from torch.onnx import export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"../logs/6_finetune_classification.log\",\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    filemode='w')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def print_time(intput_str, start_time=0):\n",
    "    print(\"{}: {} min\".format(input_str, round((time() - start_time) / 60, 2)))\n",
    "    \n",
    "# #Setting the threshold of logger to DEBUG\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "  \n",
    "# #Test messages\n",
    "# logger.debug(\"Harmless debug Message\")\n",
    "# logger.info(\"Just an information\")\n",
    "# logger.warning(\"Its a Warning\")\n",
    "# logger.error(\"Did you try to divide by zero\")\n",
    "# logger.critical(\"Internet is down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1930 entries, 0 to 1929\n",
      "Data columns (total 13 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   title                      1930 non-null   object\n",
      " 1   selftext                   1930 non-null   object\n",
      " 2   author                     1930 non-null   object\n",
      " 3   score                      1930 non-null   int64 \n",
      " 4   num_comments               1930 non-null   int64 \n",
      " 5   is_anxiety                 1930 non-null   int64 \n",
      " 6   url                        1930 non-null   object\n",
      " 7   selftext_clean             1930 non-null   object\n",
      " 8   selftext_broken_sentences  1930 non-null   object\n",
      " 9   selftext_broken_words      1930 non-null   object\n",
      " 10  title_clean                1930 non-null   object\n",
      " 11  author_clean               1930 non-null   object\n",
      " 12  megatext_clean             1930 non-null   object\n",
      "dtypes: int64(3), object(10)\n",
      "memory usage: 196.1+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_anxiety</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>selftext_broken_sentences</th>\n",
       "      <th>selftext_broken_words</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>2319</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>['we understand that most people who reply imm...</td>\n",
       "      <td>['understand', 'people', 'reply', 'immediately...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regular Check-In Post, with important reminder...</td>\n",
       "      <td>Welcome to /r/depression's check-in post - a p...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>312</td>\n",
       "      <td>1136</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/m...</td>\n",
       "      <td>welcome r depression check post place take mom...</td>\n",
       "      <td>[\"welcome to /r/depression's check-in post - a...</td>\n",
       "      <td>['welcome', 'r', 'depression', 'check', 'post'...</td>\n",
       "      <td>regular check post important reminder private ...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>sql witch welcome r depression check post plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Low</td>\n",
       "      <td>I'm so low rn I can't even type anything coher...</td>\n",
       "      <td>RagingFlock89</td>\n",
       "      <td>263</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/n...</td>\n",
       "      <td>low rn even type anything coherent want expres...</td>\n",
       "      <td>[\"i'm so low rn i can't even type anything coh...</td>\n",
       "      <td>['low', 'rn', 'even', 'type', 'anything', 'coh...</td>\n",
       "      <td>low</td>\n",
       "      <td>raging flock 89</td>\n",
       "      <td>raging flock 89 low rn even type anything cohe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "1  Regular Check-In Post, with important reminder...   \n",
       "2                                                Low   \n",
       "\n",
       "                                            selftext         author  score  \\\n",
       "0  We understand that most people who reply immed...       SQLwitch   2319   \n",
       "1  Welcome to /r/depression's check-in post - a p...       SQLwitch    312   \n",
       "2  I'm so low rn I can't even type anything coher...  RagingFlock89    263   \n",
       "\n",
       "   num_comments  is_anxiety  \\\n",
       "0           175           0   \n",
       "1          1136           0   \n",
       "2            43           0   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/depression/comments/d...   \n",
       "1  https://www.reddit.com/r/depression/comments/m...   \n",
       "2  https://www.reddit.com/r/depression/comments/n...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "1  welcome r depression check post place take mom...   \n",
       "2  low rn even type anything coherent want expres...   \n",
       "\n",
       "                           selftext_broken_sentences  \\\n",
       "0  ['we understand that most people who reply imm...   \n",
       "1  [\"welcome to /r/depression's check-in post - a...   \n",
       "2  [\"i'm so low rn i can't even type anything coh...   \n",
       "\n",
       "                               selftext_broken_words  \\\n",
       "0  ['understand', 'people', 'reply', 'immediately...   \n",
       "1  ['welcome', 'r', 'depression', 'check', 'post'...   \n",
       "2  ['low', 'rn', 'even', 'type', 'anything', 'coh...   \n",
       "\n",
       "                                         title_clean     author_clean  \\\n",
       "0  broken least understood rule helper may invite...        sql witch   \n",
       "1  regular check post important reminder private ...        sql witch   \n",
       "2                                                low  raging flock 89   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  \n",
       "1  sql witch welcome r depression check post plac...  \n",
       "2  raging flock 89 low rn even type anything cohe...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = pd.read_csv('../data/data_for_model.csv', keep_default_na=False)\n",
    "print(model_data.info())\n",
    "model_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = \"selftext_clean\"\n",
    "labels = {1: \"anxiety\", 0: \"depression\"}\n",
    "labels_str2idx = {'depression': 0, 'anxiety': 1}\n",
    "# model_data[\"megatext_clean\"].to_csv(data_path, header=None, index=None, sep='\\t', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 128\n",
    "# max sequence length for each document/sentence sample\n",
    "max_length = 512\n",
    "EPOCHS = 10\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, val_text, train_labels, val_labels = train_test_split(\n",
    "    model_data[data_column].tolist(), \n",
    "    model_data[\"is_anxiety\"].tolist(), \n",
    "    test_size=.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/bert_uncased_L-4_H-512_A-8\"\n",
    "# load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "# tokenize the dataset, truncate when passed `max_length`, \n",
    "# and pad with 0's when less than `max_length`\n",
    "train_encodings = tokenizer(train_text, truncation=True, padding=True, max_length=max_length)\n",
    "val_encodings = tokenizer(val_text, truncation=True, padding=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# convert our tokenized data into a torch Dataset\n",
    "train_dataset = RedditDataset(train_encodings, train_labels)\n",
    "val_dataset = RedditDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-512_A-8 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-512_A-8 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the model and pass to CUDA\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(labels)).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/bert_classification_lm', # output directory\n",
    "    evaluation_strategy=\"epoch\",                   # Evaluation is done at the end of each epoch.\n",
    "    num_train_epochs=EPOCHS,                       # total number of training epochs\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,  # batch size per device during training\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,    # batch size for evaluation\n",
    "    warmup_steps=500,                              # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                             # strength of weight decay\n",
    "    logging_dir ='../logs/bert_classification_lm', # directory for storing logs\n",
    "    logging_steps=200,                             # log & save weights each logging_steps\n",
    "    load_best_model_at_end=True,                   # load the best model when finished training (default metric is loss) \n",
    "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "    save_total_limit=1,                            # limit the total amount of checkpoints. Deletes the older checkpoints.    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1930' max='1930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1930/1930 03:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.446038</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>325.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.606900</td>\n",
       "      <td>0.370412</td>\n",
       "      <td>1.188600</td>\n",
       "      <td>324.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.374500</td>\n",
       "      <td>0.347635</td>\n",
       "      <td>1.196000</td>\n",
       "      <td>322.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.534144</td>\n",
       "      <td>1.199100</td>\n",
       "      <td>321.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.571521</td>\n",
       "      <td>1.201800</td>\n",
       "      <td>321.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.631738</td>\n",
       "      <td>1.205400</td>\n",
       "      <td>320.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.716783</td>\n",
       "      <td>1.207600</td>\n",
       "      <td>319.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.753405</td>\n",
       "      <td>1.210900</td>\n",
       "      <td>318.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.778459</td>\n",
       "      <td>1.210700</td>\n",
       "      <td>318.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>0.779626</td>\n",
       "      <td>1.211300</td>\n",
       "      <td>318.662000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1930, training_loss=0.1956141222326249, metrics={'train_runtime': 191.3326, 'train_samples_per_second': 10.087, 'total_flos': 1364356812472320, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics1(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def compute_metrics2(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.779626190662384,\n",
       " 'eval_accuracy': 0.8730569948186528,\n",
       " 'eval_runtime': 1.2255,\n",
       " 'eval_samples_per_second': 314.975}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics1\n",
    ")\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/bert_classification_lm/tokenizer_config.json',\n",
       " '../models/bert_classification_lm/special_tokens_map.json',\n",
       " '../models/bert_classification_lm/vocab.txt',\n",
       " '../models/bert_classification_lm/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"../models/bert_classification_lm\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload our model/tokenizer. Optional, only usable when in Python files instead of notebooks\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(labels)).to(\"cuda\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(text):\n",
    "    # prepare our text into tokenized sequence\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # perform inference to our model\n",
    "    outputs = model(**inputs)\n",
    "    # get output probabilities by doing softmax\n",
    "    probs = outputs[0].softmax(1)\n",
    "    # executing argmax function to get the candidate label\n",
    "#     return labels[probs.argmax().item()]\n",
    "    return probs.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed_text</th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tired tired fighting live care anything tired ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know one perfect life bad like literally every...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 ive always attracted woman think pocd destr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love sport play never coached never played spo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           seed_text  predicted  actual\n",
       "0  tired tired fighting live care anything tired ...          0       0\n",
       "1  know one perfect life bad like literally every...          0       0\n",
       "2  17 ive always attracted woman think pocd destr...          1       1\n",
       "3  love sport play never coached never played spo...          1       1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_count = 2\n",
    "results = []\n",
    "\n",
    "for label_str, label_int in labels_str2idx.items():\n",
    "    data = model_data[model_data[\"is_anxiety\"] == label_int][data_column]\n",
    "    \n",
    "    for i in range(exp_count):\n",
    "        seed_text = data[randint(data.index[0], data.index[-1]+1)]\n",
    "\n",
    "        generated = get_prediction(seed_text)\n",
    "        \n",
    "        model_results = {}\n",
    "        model_results[\"seed_text\"] = seed_text\n",
    "        model_results[\"predicted\"] = generated\n",
    "        model_results[\"actual\"] = label_int\n",
    "\n",
    "        results.append(model_results) \n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "964d23c0451dcf6eb9653ab29bf1b69d4b9cf2f8bbce59b0642e3dd73cc18be8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
